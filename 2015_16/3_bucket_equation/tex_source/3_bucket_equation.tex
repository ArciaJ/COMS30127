\documentclass[11pt,a4paper]{scrartcl}
\typearea{12}
\usepackage{graphicx}
\usepackage{pstricks}
\usepackage{listings}
\usepackage{amsmath}
\lstset{language=python}
\pagestyle{headings}
\markright{Computation Neuroscience - 3 the bucket equation}
\begin{document}

\subsection*{Introduction}
These notes are about first order ordinary differential
equations. These equations appear frequently in neuroscience so it is
useful to consider them now before going any further.

\subsection*{Buckets of water}

In the simplest model of neurons their voltage dynamics is similar to
the dynamics of a bucket with a leak and the class of equations that
apply in this case will also be applied to synapses, for example.

\begin{center}
\setlength{\unitlength}{2mm}
\begin{picture}(20,40)
\linethickness{0.3mm}
\put(7,5){\line(1,0){8}}
\put(15,5){\line(0,1){20}}
\put(5,5){\line(0,1){20}}
\put(15,25){\line(-1,0){10}}
\put(6,5){\vector(0,-1){3}}
\put(10,27){\vector(0,-1){5}}
\put(17,10.5){\vector(0,-1){5.5}}
\put(17,14.5){\vector(0,1){5.5}}
\linethickness{0.075mm}
\put(5,20){\line(1,0){10}}
\put(7,2){$l(t)$}
\put(11,27){$i(t)$}
\put(16,12){$h(t)$}
\end{picture}
\end{center}

Consider a bucket with straight sides which is filled to a height $h$
with water. Imagine the water leaks out of a hole in the bottom. The
rate the water leaks out depends on $h$; the larger $h$ is the larger
the pressure at the bottom is and hence the faster the water pours
out. In other words
\begin{equation}
l(t)\propto h(t)
\end{equation}
or 
\begin{equation}
l(t)= G h(t)
\end{equation}
where $G$ is a constant which will depend on the size of the hole and
complicated things like the viscosity of water. Of course, we are also
ignore lots of complicated stuff, like turbulence and so forth, but
since we are interested in the equation rather than the amount of
water in a bucket, this is fine. Imagine water also pours in the top
at a rate $i(t)$. This means the total rate of change of the amount of
water is $i(t)-Gh(t)$.

Now, $h(t)$ is the height of the water not the volume: the volume is
$Ch(t)$ where $C$ is the cross-sectional area of the bucket. The rate
of change of the volume is therefore
\begin{equation}
\frac{dCh(t)}{dt}=i(t)-Gh(t)
\end{equation}
or
\begin{equation}
\frac{dh}{dt}=\frac{1}{C}(i-Gh)
\end{equation}

Let's solve this equation for constant $i$ before going on to look at
neurons. Probably the best to do this is by using an integrating
factor, let $\tau=C/G$ and $\tilde{\i}=i/G$
\begin{equation}
\tau\frac{dh}{dt}+h=\tilde{\i}
\end{equation}
then we multiply across by $\exp{t/\tau}$ and divide by $\tau$
\begin{equation}
e^{t/\tau}\frac{dh}{dt}+\frac{1}{\tau}e^{t/\tau}h=\frac{\tilde{\i}e^{t/\tau}}{\tau}
\end{equation}
Now we can rewrite the left hand side using the product rule
\begin{equation}
\frac{d}{dx}(uv)=u\frac{dv}{dx}+v\frac{du}{dx}
\end{equation}
to give
\begin{equation}
\frac{d}{dt}\left(e^{t/\tau}h\right)=\frac{\tilde{\i}e^{t/\tau}}{\tau}
\end{equation}
Now integrating both sides gives
\begin{equation}
e^{t/\tau}h=\tilde{\i}e^{t/\tau}+A
\end{equation}
where $A$ is an integration constant. This gives
\begin{equation}
h=Ae^{-t/\tau}+\tilde{\i}
\end{equation}
and putting $t=0$ shows $A=h(0)-\tilde{\i}$ so
\begin{equation}
h(t)=[h(0)-\tilde{\i}]e^{-t/\tau}+\tilde{\i}
\end{equation}
so, basically, the value of $h$ decays exponentially until it
equilibriates with $\tilde{\i}$.

These dynamics make good intuitive sense; the more water there is in
the bucket, the higher the pressure will be at the leak and the
quicker the water will pour out. If there is just the right amount of
water the rate the water pours out the leak will precisely match the
rate it pours in, this is the equilibrium. If there is more water than
required for equilibrium it will pour out faster than the flow coming
in, if there is less, it will pour out slower. Either way, as time
passes the height of the water will reach the equilibrium. The plot in
Fig.~\ref{bucket_v} illustrates this.

\begin{figure}
\begin{center}
\include{bucket_v}
\end{center}
\caption{Exponential relaxation. The dynamics described by the
  \lq{}bucket equation\rq{} are very common. Here
  $h=[h(0)-\tilde{\i}]\exp(-t/\tau)+\tilde{\i}$ is plotted with
  $\tilde{\i}=1$, $\tau=1$ and three different values of
  $h(0)$. $h(t)$ relaxes towards the equilibrium value $\tilde{\i}=1$,
  the closer it gets, the slower it approaches.\label{bucket_v}}
\end{figure}

\subsection*{Variable input}

We have only discussed constant inputs; the variable input case where
$i$ depends on time is harder and although it can sometimes be solved
it is often easier just to compute it numerically, we will look
briefly at how to do this, but first note that the effect of variable
input is that the solution kind of chases the input with a timescale
set by $\tau$. For very small $\tau$ is chases it quickly, so it is
close to the input, but for large $\tau$ it lags behind it and smooths
it out. This is sometimes described by saying that it \textsl{filters}
the input. There is an illustration in Fig.~\ref{chasing}.

\begin{figure}
\begin{center}
\include{chasing}
\end{center}
\caption{Variable input. Here the input is a sine wave $\tilde{\i}=\sin{t}$ and the equation is evolved with $h(0)$ and three different $\tau$ values. For $\tau=0.25$ we see $h(t)$ closely matches the input whereas for larger $\tau$ it is smoother and lags behind.\label{chasing}}
\end{figure}

\subsection*{Numerical solutions}

Consider the problem of solving a general first order differential equation
\begin{equation}
\frac{df(t)}{dt}=F(f,t)
\end{equation}
This includes our bucket case if we replace $f$ with $h$ and $F(f,t)$
with $(i-Gh)/C$. Now consider the situation where we want to solve
this but can't do it analytically, as will be the case for many
values of the time dependent input $i(t)$ or, indeed, if $i(t)$ itself
is only known numerically. 

The simplest standard numerical approach is Euler's method. Say you
know the value of $f(t)$ and want to approximate $f(t+\delta t)$ where
$\delta t$ is some small increment in time. Now, we know from the
Taylor expansion that
\begin{equation}
f(t+\delta t)=f(t)+\delta t \frac{df(t)}{dt}+\frac{1}{2}\delta t^2 \frac{d^2f(t)}{dt^2}+O(\delta t^3)
\end{equation}
Now we know what $df(t)/dt$ is, it is $F(f,t)$ from the equation, so 
\begin{equation}
f(t+\delta t)=f(t)+\delta t F(f,t) + O(\delta t^2)
\end{equation}
so approximating $f(t+\delta t)$ with $f(t)+\delta t F(f,t)$ is actual
to first order in the time step $\delta t$. This is the Euler
approximation. More formally if we have an initial condition $f(t_0)$
and write
\begin{align}
f_n&=f(t_0+n\delta t)\\
t_n&=t_0+n\delta t
\end{align}
then the Euler approximation is
\begin{equation}
f_{n+1}=f_n+\delta t F(f_n,t_n)
\end{equation}
Thus, in the case we are interested in here
\begin{equation}
h_{n+1}=\frac{[i(t_n)-Gh_n]\delta t}{C}
\end{equation}

Now this approximation is often good enough for neuroscience,
especially if as often happens the errors are sometimes positive and
sometimes negative. However, it can also happen that they can add up
and the $O(\delta t^2)$ error can be a problem. The way around this is
to use Runge-Kutta. Roughly, for Runge-Kutta a number of different
Taylor expansions are considered, using not just the expansion with
$\delta t$ but also $\delta t /2$ and combining them in such a way as
to get an approximation that is accurate to a higher order in 
$\delta t$. In other words it combines different quantities so that the
coefficients of $\delta t^2$ and so on cancel. The Euler method can be
thought of as a first order Runge Kutta method, the method that is
used most often is fourth order Runge Kutta, the error for this is
$O(\delta t^5)$. Explicitly, see for example \cite{}, first define
$k_1$ through to $k_4$:
\begin{align}
k_1&=\delta t F(f_n,t_n)\\	
k_2&=\delta t F(f_n+k_1/2,t_n+\delta t/2)\\	
k_3&=\delta t F(f_n+k_2/2,t_n+\delta t/2)\\	
k_4&=\delta t F(f_n+k_3,t_n+\delta t)	
\end{align}
and then
\begin{equation}
f_(n+1)	=f_n+\frac{1}{6}k_1+\frac{1}{3}k_2+\frac{1}{3}k_3+\frac{1}{6}k_4
\end{equation}

In fact, in neuroscience, we sometimes have to deal with what are
called \textsl{stiff} differential equations; these are equations
where change is sometimes very rapid and sometimes much
slower. Obviously the bucket equations discussed for far don't have
this property, in fact they only have one timescale, $\tau$, but later
we will example the Hodgkin-Huxley equations, which do. We will not
consider the problem of stiff equations in the course, but there are
methods, such as the \textsl{backwards Euler} method that deal well
with stiff equations and the standard neuron simulation packages, like
NEURON and GENESIS, and BRIAN with Python, are optimized to the
particular properties of the differential equations governing neuronal
dynamics.

\begin{thebibliography}{10}

\bibitem{PressEtAl1988}
Press WH, Teukolsky SA, Vetterling WT and Flannery BP. (1988) "Numerical recipes in C." Cambridge University Press.

\end{thebibliography}

\end{document}

